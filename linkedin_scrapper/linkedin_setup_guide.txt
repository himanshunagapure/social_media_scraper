# LinkedIn Scraper Setup Guide

## Prerequisites

- Python 3.8 or higher
- LinkedIn account with valid credentials
- Stable internet connection

## 1. Installation

### Step 1: Create Virtual Environment
```bash
# Create virtual environment
python -m venv linkedin_scraper_env

# Activate virtual environment
# On Windows:
linkedin_scraper_env\Scripts\activate
# On macOS/Linux:
source linkedin_scraper_env/bin/activate
```

### Step 2: Install Required Dependencies
```bash
# Install the unofficial LinkedIn API
pip install linkedin-api

# Install additional dependencies
pip install requests urllib3 pathlib
```

### Step 3: Alternative Installation (if above fails)
```bash
# If linkedin-api doesn't work, try this fork:
pip install git+https://github.com/nsandman/linkedin-api.git
```

## 2. Configuration

### Step 1: Create Configuration File
Create a file named `config.json` in the same directory as your scraper:

```json
{
  "username": "your_linkedin_email@example.com",
  "password": "your_linkedin_password",
  "min_delay": 5,
  "max_delay": 10,
  "max_requests_per_hour": 50,
  "proxy": null
}
```

### Step 2: Optional - Configure Proxy (Recommended)
If you want to use a proxy for additional security:

```json
{
  "username": "your_linkedin_email@example.com",
  "password": "your_linkedin_password",
  "min_delay": 5,
  "max_delay": 10,
  "max_requests_per_hour": 50,
  "proxy": "http://proxy-server:port"
}
```

## 3. Usage Examples

### Basic Usage
```python
from linkedin_scraper import LinkedInScraper

# Initialize scraper
scraper = LinkedInScraper()

# Authenticate
if scraper.authenticate():
    print("Authentication successful!")
    
    # Get a profile
    profile = scraper.get_profile('williamhgates')
    if profile:
        print(f"Profile: {profile['firstName']} {profile['lastName']}")
        
    # Search for people
    results = scraper.search_people('python developer', limit=10)
    print(f"Found {len(results)} people")
    
    # Health check
    health = scraper.health_check()
    print(f"Scraper health: {health}")
```

### Advanced Usage
```python
# Scrape multiple profiles from search
profiles = scraper.scrape_profiles_from_search('data scientist', max_profiles=20)

# Save data
scraper.save_data(profiles, 'data_scientists.json')

# Get company information
company = scraper.get_company('microsoft')
if company:
    print(f"Company: {company['name']}")
```

## 4. Security Features

### Rate Limiting
- Automatic delays between requests (5-10 seconds by default)
- Hourly request limits (50 requests/hour by default)
- Randomized delays to mimic human behavior

### Session Management
- Automatic session caching and reuse
- Session persistence across runs
- Automatic session refresh when expired

### Anti-Detection Measures
- User-agent rotation
- Proxy support
- Request retry mechanism
- Proper error handling

## 5. File Structure

Your project directory should look like this:
```
linkedin_scraper/
├── linkedin_scraper.py        # Main scraper code
├── config.json               # Configuration file
├── linkedin_session.pkl      # Session cache (auto-generated)
├── linkedin_scraper.log      # Log file (auto-generated)
├── requirements.txt          # Dependencies
└── scraped_data/            # Directory for output files
    ├── profile_example.json
    ├── search_results.json
    └── scraped_profiles.json
```

## 6. Requirements.txt

Create a `requirements.txt` file with the following content:

```txt
linkedin-api>=2.0.0
requests>=2.25.0
urllib3>=1.26.0
```

## 7. Important Notes

### Legal and Ethical Considerations
- ⚠️ **This scraper is for educational purposes only**
- Only scrape publicly available information
- Respect LinkedIn's robots.txt and Terms of Service
- Do not overwhelm LinkedIn's servers
- Use reasonable delays between requests
- Consider LinkedIn's official API for commercial use

### Account Safety
- Use a dedicated LinkedIn account for scraping
- Don't use your main business account
- Monitor for any account restrictions
- Consider using residential proxies for additional protection

### Error Handling
- The scraper includes comprehensive error handling
- Check log files for detailed error information
- Authentication errors are logged and handled gracefully
- Rate limiting prevents IP bans

## 8. Troubleshooting

### Common Issues

1. **Authentication Failed**
   - Check username/password in config.json
   - Verify account isn't locked/restricted
   - Try logging in manually first

2. **Rate Limiting Errors**
   - Increase delays in config.json
   - Reduce max_requests_per_hour
   - Use proxies to distribute requests

3. **Import Errors**
   - Ensure virtual environment is activated
   - Reinstall linkedin-api: `pip install --upgrade linkedin-api`
   - Try the GitHub version: `pip install git+https://github.com/nsandman/linkedin-api.git`

4. **CAPTCHA Challenges**
   - Use proxies and rotate user agents
   - Increase delays between requests
   - Consider using CAPTCHA solving services

5. **Session Expired**
   - Delete `linkedin_session.pkl` file
   - Re-authenticate with fresh credentials
   - Check if account requires verification

### Debugging Tips

1. **Enable Detailed Logging**
   ```python
   import logging
   logging.basicConfig(level=logging.DEBUG)
   ```

2. **Check Health Status**
   ```python
   health = scraper.health_check()
   print(json.dumps(health, indent=2))
   ```

3. **Monitor Log Files**
   ```bash
   tail -f linkedin_scraper.log
   ```

## 9. Advanced Configuration

### Custom Headers
```python
# Add custom headers for additional stealth
scraper.session.headers.update({
    'X-Requested-With': 'XMLHttpRequest',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin'
})
```

### Proxy Rotation
```python
# Implement proxy rotation
proxies = [
    'http://proxy1:port',
    'http://proxy2:port',
    'http://proxy3:port'
]

import random
scraper.config['proxy'] = random.choice(proxies)
```

## 10. Performance Optimization

### Batch Processing
- Process profiles in batches of 10-20
- Implement checkpointing for large datasets
- Use concurrent processing (carefully)

### Memory Management
- Clear large data structures after use
- Implement data streaming for large results
- Monitor memory usage during long runs

## 11. Output Formats

### JSON Output
Default format with complete profile data:
```json
{
  "firstName": "John",
  "lastName": "Doe",
  "headline": "Software Engineer",
  "location": "San Francisco",
  "scraped_at": "2025-07-17T10:30:00",
  "search_keywords": "python developer"
}
```

### CSV Export
```python
import pandas as pd

# Convert JSON to CSV
df = pd.DataFrame(profiles)
df.to_csv('profiles.csv', index=False)
```

## 12. Best Practices

1. **Always test with small datasets first**
2. **Use delays appropriate for your use case**
3. **Monitor your account for any restrictions**
4. **Keep your scraper updated**
5. **Respect rate limits and server capacity**
6. **Back up your session files**
7. **Use version control for your configurations**

## Support

If you encounter issues:
1. Check the log files for detailed error messages
2. Verify your LinkedIn account is in good standing
3. Ensure all dependencies are properly installed
4. Test with a simple profile lookup first

Remember: This tool is for educational and research purposes only. Always comply with LinkedIn's Terms of Service and applicable laws in your jurisdiction.